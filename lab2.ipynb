{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "url = \"https://visitseattle.org/events/page/\" # + page number, from 1 to 41\n",
    "\n",
    "# loop pages\n",
    "for i in range(1, 42):\n",
    "    res = requests.get(url + str(i) + \"/\")\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    selector = \"div.search-result-preview > div > h3 > a\"\n",
    "    link = soup.select(selector)\n",
    "    links.extend([x['href'] for x in link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape detailed pages\n",
    "details = [[\"Name\", \"Date\", \"Location\", \"Type\", \"Region\"]]\n",
    "\n",
    "for link in links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    data_item = [] # store data for one detailed page\n",
    "    # extract names\n",
    "    name_selector = \"div.medium-6.columns.event-top > h1\"\n",
    "    name = soup.select(name_selector)\n",
    "    data_item.append(name[0].contents[0].strip())\n",
    "    # extract dates\n",
    "    date_selector = \"div.medium-6.columns.event-top > h4 > span:nth-child(1)\"\n",
    "    date = soup.select(date_selector)\n",
    "    data_item.append(date[0].contents[0].strip())\n",
    "    # extract locations\n",
    "    location_selector = \"div.medium-6.columns.event-top > h4 > span:nth-child(2)\"\n",
    "    location = soup.select(location_selector)\n",
    "    data_item.append(location[0].contents[0].strip())\n",
    "    # extract types\n",
    "    type_selector = \"div.medium-6.columns.event-top > a:nth-child(3)\"\n",
    "    type = soup.select(type_selector)\n",
    "    data_item.append(type[0].contents[0].strip())\n",
    "    # extract regions\n",
    "    region_selector = \"div.medium-6.columns.event-top > a:nth-child(4)\"\n",
    "    region = soup.select(region_selector)\n",
    "    data_item.append(region[0].contents[0].strip())\n",
    "\n",
    "    details.append(data_item) # append data for one detailed page to the list\n",
    "    # time.sleep(1)  # sleep 1 second to avoid being blocked by the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('events.csv', 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerows(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queen Anne / Seattle Center\n",
      "{'q': 'Seattle Center, Seattle', 'format': 'jsonv2'}\n",
      "Queen Anne / Seattle Center\n",
      "{'q': 'Seattle Center, Seattle', 'format': 'jsonv2'}\n",
      "Bellevue / Eastside\n",
      "{'q': 'Bellevue Way NE, Seattle', 'format': 'jsonv2'}\n",
      "Bellevue / Eastside\n",
      "{'q': 'Bellevue Way NE, Seattle', 'format': 'jsonv2'}\n",
      "Queen Anne / Seattle Center\n",
      "{'q': 'Seattle Center, Seattle', 'format': 'jsonv2'}\n",
      "Queen Anne / Seattle Center\n",
      "{'q': 'Seattle Center, Seattle', 'format': 'jsonv2'}\n",
      "Queen Anne / Seattle Center\n",
      "{'q': 'Seattle Center, Seattle', 'format': 'jsonv2'}\n",
      "Queen Anne / Seattle Center\n",
      "{'q': 'Seattle Center, Seattle', 'format': 'jsonv2'}\n",
      "Bellevue / Eastside\n",
      "{'q': 'Bellevue Way NE, Seattle', 'format': 'jsonv2'}\n",
      "Fremont / Ballard\n",
      "{'q': ' 5345 Ballard Ave NW, Seattle', 'format': 'jsonv2'}\n",
      "Fremont / Ballard\n",
      "{'q': '3401 Evanston Ave N, Seattle', 'format': 'jsonv2'}\n"
     ]
    }
   ],
   "source": [
    "# scrape coordinates\n",
    "coordinate_data =[] # stores latitude and longitude\n",
    "location_cannotfind = [] # stores locations that cannot be found\n",
    "\n",
    "for i in range(1, len(details)):\n",
    "    cord_base_url = \"https://nominatim.openstreetmap.org/search.php\"\n",
    "    try: # first try to find the location with the location name\n",
    "        # search parameters\n",
    "        query_params = {\n",
    "            \"q\": details[i][2] + \", Seattle\",\n",
    "            \"format\": \"jsonv2\"\n",
    "        }\n",
    "        # specify headers\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'Referer': 'https://nominatim.openstreetmap.org/ui/search.html'\n",
    "        }\n",
    "        res_cord = requests.get(cord_base_url, params=query_params, headers=headers)\n",
    "        res_dict = res_cord.json()\n",
    "        coordinate_data.append([res_dict[0]['lat'], res_dict[0]['lon']])\n",
    "    except:\n",
    "        try: # then try to find location with region name\n",
    "            # search parameters\n",
    "            query_params = {\n",
    "                \"q\": details[i][4] + \", Seattle\",\n",
    "                \"format\": \"jsonv2\"\n",
    "            }\n",
    "            # specify headers\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "                'Referer': 'https://nominatim.openstreetmap.org/ui/search.html'\n",
    "            }\n",
    "            res_cord = requests.get(cord_base_url, params=query_params, headers=headers)\n",
    "            res_dict = res_cord.json()\n",
    "            coordinate_data.append([res_dict[0]['lat'], res_dict[0]['lon']])\n",
    "        except: # if still cannot find the location, redirect the location name\n",
    "            try: \n",
    "                print(details[i][4])\n",
    "                # redirect region names from experience\n",
    "                region_redirect = {\n",
    "                    \"Queen Anne / Seattle Center\": \"Seattle Center\",\n",
    "                    \"Bellevue / Eastside\": \"Bellevue Way NE\",\n",
    "                    \"Fremont / Ballard\": {\"Ballard Farmers Market\": \" 5345 Ballard Ave NW\", \"Fremont Farmers Market\": \"3401 Evanston Ave N\"}\n",
    "                }\n",
    "                if details[i][4] in region_redirect.keys(): # if the region name is in the redirect list\n",
    "                    if details[i][4] == \"Fremont / Ballard\": # if the region name is a dict\n",
    "                        if details[i][2] in region_redirect[details[i][4]].keys(): # if the location name is in the redirect list\n",
    "                            query_params = {\n",
    "                                \"q\": region_redirect[details[i][4]][details[i][2]] + \", Seattle\",\n",
    "                                \"format\": \"jsonv2\"\n",
    "                            }\n",
    "                    else:\n",
    "                        query_params = {\n",
    "                            \"q\": region_redirect[details[i][4]] + \", Seattle\",\n",
    "                            \"format\": \"jsonv2\"\n",
    "                        }\n",
    "                    print(query_params)\n",
    "                # specify headers\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "                    'Referer': 'https://nominatim.openstreetmap.org/ui/search.html'\n",
    "                }\n",
    "                res_cord = requests.get(cord_base_url, params=query_params, headers=headers)\n",
    "                res_dict = res_cord.json()\n",
    "                coordinate_data.append([res_dict[0]['lat'], res_dict[0]['lon']])\n",
    "            except:\n",
    "                # coordinate_data.append([])\n",
    "                location_cannotfind.append([details[i][2], details[i][4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(location_cannotfind) # zero locations cannot be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the coordinate data to a csv file\n",
    "with open('locations.csv', 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerows(coordinate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastable_date = [\"1/17/2024\", \"1/18/2024\", \"1/19/2024\", \"1/20/2024\", \"1/21/2024\", \"1/22/2024\", \"1/23/2024\"] # the max date can be forecasted is '1/23/2024'\n",
    "weather_base_url = \"https://api.weather.gov/points/\"\n",
    "weather_data = [[\"Name\", \"Date\", \"Location\", \"Type\", \"Region\", \"Weather\"]]\n",
    "\n",
    "for i in range(0, len(details)):\n",
    "    if details[i][1] in forecastable_date: # filter out the events within 7 days\n",
    "        res_weather = requests.get(weather_base_url + str(coordinate_data[i][0]) + \",\" + str(coordinate_data[i][1]))\n",
    "        res_dict = res_weather.json()\n",
    "\n",
    "        res_weather_detail = requests.get(res_dict['properties']['forecast']) # use the forecast property to get the weather data\n",
    "        res_dict = res_weather_detail.json()\n",
    "\n",
    "        # convert the date format e.g. '1/17/2024' to '2024-01-17'\n",
    "        parts = details[i][1].split('/') \n",
    "        formatted_date = f\"{parts[2]}-{parts[0].zfill(2)}-{parts[1].zfill(2)}\"\n",
    "        \n",
    "        for period in res_dict[\"properties\"][\"periods\"]:\n",
    "            # we are interested in the weather data of the day time of that day\n",
    "            if period[\"startTime\"].startswith(formatted_date) and period[\"endTime\"].startswith(formatted_date):\n",
    "                weather_data.append(details[i] + [period[\"detailedForecast\"]]) # there are many attributes in the weather data, we are only interested in the detailedForecast\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the coordinate data to a csv file\n",
    "with open('weather.csv', 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerows(weather_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
